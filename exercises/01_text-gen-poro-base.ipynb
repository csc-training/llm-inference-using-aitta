{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a5f00d1-1eb9-4a15-8c45-9c80949fdbff",
   "metadata": {},
   "source": [
    "# Text generation with LumiOpen/Poro-34B model\n",
    "\n",
    "We are going to start with a so-called \"base model\", pre-trained [**LumiOpen/Poro-34B**](https://huggingface.co/LumiOpen/Poro-34B). The task is to generate text with it. We are going to use API provided by Aitta inference platform. We need a Python client `aitta-client` to be able to use it. It has been already installed to this workspace. To see it's documentation, visit [PyPi](https://pypi.org/project/aitta-client/).\n",
    "\n",
    "We also need the API key. You can create it after logging into [Aitta](https://staging-aitta.2.rahtiapp.fi/public).\n",
    "\n",
    "<details open>\n",
    "<summary>Introduction for API key creation</summary>\n",
    "<br>\n",
    "1. Log in to the web frontend  \n",
    "<br>\n",
    "2. Navigate to the model page of the model for which to generate the token  \n",
    "<br>\n",
    "3. Open the tab titled 'API Key'  \n",
    "<br>\n",
    "4. Generate and copy the token   \n",
    "</details>\n",
    "\n",
    "After this we call your API key an \"access token\". We use it to configure Aitta client. Then it is possible to load model for usage. \n",
    "\n",
    "*You can save you API key for a safe place to be used in the future since it is valid over 80 days. Note that API keys are **model specific** at the moment.*\n",
    "\n",
    "**Let's start by loading library `aitta-client` and configuring Client-instance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "246ce2e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aitta-client\n",
      "  Downloading aitta_client-0.2.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting openai\n",
      "  Downloading openai-1.66.3-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: requests~=2.28 in /opt/conda/lib/python3.11/site-packages (from aitta-client) (2.32.3)\n",
      "Collecting pydantic~=2.5.1 (from aitta-client)\n",
      "  Downloading pydantic-2.5.3-py3-none-any.whl.metadata (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyJWT~=2.8.0 in /opt/conda/lib/python3.11/site-packages (from aitta-client) (2.8.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from openai) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from openai) (0.27.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.11/site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/conda/lib/python3.11/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic~=2.5.1->aitta-client)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.14.6 (from pydantic~=2.5.1->aitta-client)\n",
      "  Downloading pydantic_core-2.14.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests~=2.28->aitta-client) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests~=2.28->aitta-client) (2.2.1)\n",
      "Downloading aitta_client-0.2.0-py3-none-any.whl (13 kB)\n",
      "Downloading openai-1.66.3-py3-none-any.whl (567 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m567.4/567.4 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (351 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m351.8/351.8 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.5.3-py3-none-any.whl (381 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.14.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: pydantic-core, jiter, annotated-types, pydantic, openai, aitta-client\n",
      "Successfully installed aitta-client-0.2.0 annotated-types-0.7.0 jiter-0.9.0 openai-1.66.3 pydantic-2.5.3 pydantic-core-2.14.6\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install libraries neede for all the exercises since workspace dedicated for this course IS NOT YET DONE \n",
    "!pip install aitta-client openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5127cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your personal model-specific API key here  \n",
    "api_key = \"<API-key>\"\n",
    "\n",
    "# Security Note:  \n",
    "# In a typical setup, API keys should be stored securely using environment variables or secret management tools.  \n",
    "# However, since this temporary Jupyter notebook expires in 4 hours and does not support environment variables,  \n",
    "# we define the API key directly in this cell.  \n",
    "\n",
    "# To keep your API key safe, consider removing or clearing this cell after execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f78f557d-a761-408b-af0c-f8b289337bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed libraries and modules\n",
    "from aitta_client import Model, Client, StaticAccessTokenSource\n",
    "\n",
    "# configure Client instance with API URL and access token\n",
    "token_source = StaticAccessTokenSource(api_key)\n",
    "client = Client(\"https://api-staging-aitta.2.rahtiapp.fi\", token_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97699c78",
   "metadata": {},
   "source": [
    "## Loading the model and some Jupyter notebook tips\n",
    "\n",
    "At the moment, only model-specific tokens are available. We can still look and see which models are available through created API key. \n",
    "\n",
    "**See available methods for the created client-instance**  \n",
    "* You can see available methods for created **client-instance** in Jupyter notebook code shell using `Tab` completion. Type the instance name followed by dot - like this `client.` -  and press `Tab-button` to see available methods. We use method `get_model_list` to see available models. \n",
    "\n",
    "**View method paramaters:**\n",
    "*  Option 1: To get detailed information about the method, including its parameters and docstring, you can append a `?` to the method name. For example write `client.get_model_list?` to a code cell and run it.\n",
    "* Option 2: When you type a method, like `client.get_model_list(`, and then press `Shift + Tab`, Jupyter will display a tooltip showing the method signature, including its parameters, expected types, and a short description if available. This is really helpful to quickly see what the method requires.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dba55e68-618c-4113-803b-b8a11187c6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Try out here ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "713f8a46-2835-4cf2-851d-926f6bebb907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LumiOpen/Poro\n"
     ]
    }
   ],
   "source": [
    "# Use the get_model_list method to retrieve the list of models\n",
    "model_list = client.get_model_list()\n",
    "\n",
    "# Iterate through the model list and print the model names/IDs\n",
    "for model_id in model_list:\n",
    "    print(model_id.id)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36a86d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LumiOpen/Poro-34B model.\n",
      "Inference requests must be made as a 'application/json' POST request.\n",
      "\n",
      "Input fields:\n",
      " - 'input': The input prompt for the model to complete as a string (or list of strings for an several prompts as a batched input).\n",
      " - 'max_new_tokens': Optional. An integer. How many new tokens to generate at maximum at the end of the prompt.\n",
      " - ... other optional permissable parameters to the huggingface transformer pipeline.\n",
      "\n",
      "Example:\n",
      "\n",
      "{'input': 'Suomen paras kaupunki on', 'do_sample': true, 'max_new_tokens': 20}\n",
      "\n",
      "Output: (application/json)\n",
      " - 'output': The completed prompt as a string (or list of strings for batched inputs).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the LumiOpen/Poro model\n",
    "model = Model.load(\"LumiOpen/Poro\", client)\n",
    "\n",
    "print(model.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ff59b7f-06e2-4dd2-bb50-f84dce466cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT:\n",
      "{'input': 'Suomen paras kaupunki on'}\n"
     ]
    },
    {
     "ename": "APIErrorResponse",
     "evalue": "The API responded with error 'too_many_requests': You have exceeded the request rate limit.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIErrorResponse\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINPUT:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# start the inference and wait for completion\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_and_await_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOUTPUT:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/aitta_client/wrappers.py:86\u001b[0m, in \u001b[0;36mModel.start_and_await_inference\u001b[0;34m(self, inputs, params, timeout_ms, polling_interval_ms)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mcompleted:\n\u001b[1;32m     85\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(polling_interval_secs)\n\u001b[0;32m---> 86\u001b[0m     \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end_time \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m<\u001b[39m end_time:\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mTimeout()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/aitta_client/wrappers.py:193\u001b[0m, in \u001b[0;36mTask.update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Makes a query to the API to update the task information. \"\"\"\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m     new_task \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_task\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data \u001b[38;5;241m=\u001b[39m new_task\u001b[38;5;241m.\u001b[39m_data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/aitta_client/client.py:132\u001b[0m, in \u001b[0;36mClient.get_task\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_task\u001b[39m(\u001b[38;5;28mself\u001b[39m, task: TaskData) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TaskData:\n\u001b[1;32m    122\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Obtain updated model inference task data.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    This is a low-level implementation of API requests. You probably want to use `Task.update` instead.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m        a `TaskData` instance containing current information about the task\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl_endpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m         task_data \u001b[38;5;241m=\u001b[39m data_structures\u001b[38;5;241m.\u001b[39mTask\u001b[38;5;241m.\u001b[39mmodel_validate_json(response\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/aitta_client/client.py:51\u001b[0m, in \u001b[0;36mClient._make_request\u001b[0;34m(self, endpoint, json, method)\u001b[0m\n\u001b[1;32m     49\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_endpoint_to_url(endpoint)\n\u001b[1;32m     50\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mrequest(method, url, headers\u001b[38;5;241m=\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_ms, json\u001b[38;5;241m=\u001b[39mjson)\n\u001b[0;32m---> 51\u001b[0m \u001b[43mhandle_error_responses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/aitta_client/api/errors.py:49\u001b[0m, in \u001b[0;36mhandle_error_responses\u001b[0;34m(response, content_type)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m:\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m AuthorizationError(error_data)\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIErrorResponse(error_data)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pydantic\u001b[38;5;241m.\u001b[39mValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m500\u001b[39m:\n",
      "\u001b[0;31mAPIErrorResponse\u001b[0m: The API responded with error 'too_many_requests': You have exceeded the request rate limit."
     ]
    }
   ],
   "source": [
    "# declare inputs and parameters for a text completion inference task\n",
    "inputs = {\n",
    "    'input': 'Suomen paras kaupunki on'\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'do_sample': True,\n",
    "    'max_new_tokens': 20\n",
    "}\n",
    "\n",
    "print(f\"INPUT:\\n{inputs}\")\n",
    "\n",
    "# start the inference and wait for completion\n",
    "result = model.start_and_await_inference(inputs, params)\n",
    "print(f\"OUTPUT:\\n{result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638c6c8a-4d4b-440b-8274-8625e662ed4e",
   "metadata": {},
   "source": [
    "## Fine-Tuning generation parameters\n",
    "\n",
    "You can customize text generation by adding parameters to the `params` dictionary in the example code. The following options are currently supported when using the `start_and_await_inference` method and align with those used in [Hugging Face’s Transformers module](https://huggingface.co/docs/transformers/main_classes/text_generation) for text generation:\n",
    "\n",
    "**Controlling output length**\n",
    "* max_new_tokens\n",
    "* min_new_tokens\n",
    "* min_length\n",
    "* max_length\n",
    "\n",
    "**Adjusting the generation strategy**\n",
    "* do_sample: \n",
    "* num_beams\n",
    "* top_k\n",
    "\n",
    "**Modifying model output behavior**\n",
    "* temperature\n",
    "* top_p\n",
    " \n",
    "For more details on how these parameters work—including their minimum and maximum values, data types, and how certain parameters override others—refer to the [Hugging Face’s documentation](https://huggingface.co/docs/transformers/main_classes/text_generation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ffca60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare inputs and parameters for a text completion inference task\n",
    "inputs = {\n",
    "    'input': 'Suomen paras kaupunki on'\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'do_sample': True,\n",
    "    'max_new_tokens': 20,\n",
    "    #### Add parameters here to test how they affect generated response ####\n",
    "}\n",
    "\n",
    "print(f\"INPUT:\\n{inputs}\")\n",
    "\n",
    "# start the inference and wait for completion\n",
    "result = model.start_and_await_inference(inputs, params)\n",
    "print(f\"OUTPUT:\\n{result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af3ccd3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
