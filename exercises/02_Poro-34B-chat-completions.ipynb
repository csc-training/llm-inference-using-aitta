{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a339f05c-0e31-425a-9a4a-d04259b46e63",
   "metadata": {},
   "source": [
    "# Chat completions with LumiOpen/Poro-34B-chat model \n",
    "\n",
    "Now we are ready to test chat-tuned [`LumiOpen/Poro-34B-chat`](https://huggingface.co/LumiOpen/Poro-34B-chat). We will start with a simple examples and move on to tweaking parameters that affect the output generated by the model. \n",
    "\n",
    "Poro-34B-Chat excels at:\n",
    "* Answering questions in Finnish and English\n",
    "* Following instructions in plain language\n",
    "* Generating high-quality translations\n",
    "* Writing code and assisting with development tasks\n",
    "\n",
    "Let's start with basics, generate API key for this specific model and configure Aitta client to load the model `LumiOpen/Poro-34B-chat`. Since aitta-client is **OpenAI compatible** we can configure OpenAI client to use the OpenAI compatibility endpoints. We need to adjust the parameters `api_key` and ` base_url` when configuring the OpenAI client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab906c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your personal model-specific API key here  \n",
    "api_key = \"<api-key>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72fec64-4524-494b-b316-b3f89b9e7b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aitta_client import Model, Client, StaticAccessTokenSource\n",
    "import openai\n",
    "\n",
    "\n",
    "# configure Client instance with API URL and access token\n",
    "token_source = StaticAccessTokenSource(api_key)\n",
    "aitta_client = Client(\"https://api-staging-aitta.2.rahtiapp.fi\", token_source)\n",
    "\n",
    "# load the LumiOpen/Poro-34B-chat model\n",
    "model = Model.load(\"LumiOpen/Poro-34B-chat\", aitta_client)\n",
    "print(model.description)\n",
    "\n",
    "# configure OpenAI client to use the Aitta OpenAI compatibility endpoints\n",
    "client = openai.OpenAI(api_key=token_source.get_access_token(), base_url=model.openai_api_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b9ee48-a74d-4711-81ae-f17ac650df27",
   "metadata": {},
   "source": [
    "# Create chat completions\n",
    "\n",
    "You must be eager to get started with actually getting responses from model based on your prompt.\n",
    "Let's create one using  `client.chat.completions.create()` method and then explore how to fine-tune its parameters.\n",
    "\n",
    "Here again diagram of the first API call using client for chat completions: \n",
    "\n",
    "![api-call-diagram-for-chat-completions](./images/API-chat-completions-bold.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30e73e9-46b1-454b-9846-a1192bfc4f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform chat completion with the OpenAI client\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is an API? Explain like I would be five.\"\n",
    "        }\n",
    "    ],\n",
    "    model=model.id,\n",
    "    stream=False  # response streaming is currently not supported by Aitta, now you get the full response in one go\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3c7cd6-bdbc-4a8d-89f3-af98b949eb53",
   "metadata": {},
   "source": [
    "# Understanding required parameters for `client.chat.completions.create` method\n",
    "\n",
    "We just used `client.chat.completions.create` method to generate chat-based completions. This method requires two key parameters: `messages` and `model`. \n",
    "\n",
    "The model refers to the specific AI model that you’ve loaded with the Aitta client. The messages parameter is a list of message objects that represents the conversation history up to the point of the request. Each message object includes a `role` and `content`. The role determines how the model interprets the message, and it can be one of three types: `user`, `system`, or `assistant`. \n",
    "The `content` field is the actual text of the message. Understanding the role helps the model generate more accurate and contextually appropriate responses based on the conversation flow.\n",
    "\n",
    "| Role | Description | Usage example |\n",
    "|----|----|---|\n",
    "| user | Represents the input from the user, such as a question or prompt. This helps initiate the conversation or provide queries. | `{ \"role\": \"user\", \"content\": \"How does AI work?\" }` |\n",
    "| system | Provides guidelines or instructions for how the model should behave throughout the conversation. This is useful for setting the tone or framing the conversation. | `{ \"role\": \"system\", \"content\": \"You are an assistant who always provides concise answers.\" }` |\n",
    "| assistant | Represents the AI's response to the user, guiding the conversation forward. This role helps the model generate replies based on the user's input. | `{ \"role\": \"assistant\", \"content\": \"AI works by processing large amounts of data to make predictions or decisions.\" }` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9770e296-2d7d-43c5-8382-2f43fdcfaf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example messages to prompt the model\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you explain what AI is?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"AI stands for Artificial Intelligence. It refers to machines that can perform tasks that typically require human intelligence.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How does AI work?\"}\n",
    "]\n",
    "\n",
    "# Perform a chat completion request\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=model.id,\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "# Output the response\n",
    "print(chat_completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822dbdca-66dc-4112-b212-9b34206f2593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below line to see documentation for this method\n",
    "#help(client.chat.completions.create)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc668ced-8829-452c-91b5-5cbc91c9c958",
   "metadata": {},
   "source": [
    "# Understanding optional parameters for `client.chat.completions.create` method\n",
    "\n",
    "There are several parameters you can adjust to fine-tune the generated responses. These parameters help control the randomness, length, and number of responses generated. Below are the parameters supported for OpenAI compatibility at the moment. \n",
    "\n",
    "\n",
    "| Parameter | Description | Effect |\n",
    "|-----------|-------------|--------|\n",
    "| **top_p** (nucleus sampling) | A value between 0 and 1. `top_p` controls how the model picks the next word by considering only the most likely options. | Lower values (like 0.5) make the model more focused and predictable. Higher values (like 0.9) make the model more creative and varied. |\n",
    "| **temperature** (randomness control) | A value between 0 and 2. `temperature` controls the creativity of the response. Higher values create more varied, creative responses, while lower values produce more focused, deterministic outputs. | A higher temperature increases creativity, while a lower temperature makes responses more predictable. |\n",
    "| **max_completion_tokens** (maximum tokens)| Limits the number of tokens (words or parts of words) the model can generate in a response. | A smaller value shortens the response, while a larger value allows for more detailed answers. |\n",
    "| **n** (number of completions)| A higher value (e.g., 3) generates multiple variations of responses, offering different perspectives or creative options. |\n",
    "\n",
    "You can explore how these affect responses generated by the model.  \n",
    "\n",
    "**Note that longer input prompts and higher `max_completion_tokens` can increase the time it takes to receive a response.**\n",
    "\n",
    "\n",
    "*Tip: Use `temperature` and `top_p` together cautiously. Typically, adjusting one is enough—setting both too high may lead to overly random responses, while both too low may make responses overly constrained.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4fc329-af5e-4877-b442-5ba95b486702",
   "metadata": {},
   "source": [
    "## Try it yourself\n",
    "\n",
    "Now it's time for you to experiment with these parameters.\n",
    "\n",
    "You may have noticed that sometimes responses are cut off mid-sentence. You can influence this by adjusting `max_completion_tokens`. However, even with a higher value, truncation might still occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff27b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE HERE ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
