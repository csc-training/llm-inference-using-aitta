{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c3a6e18-7899-4a48-95ae-b5b7c3a5a642",
   "metadata": {},
   "source": [
    "# Tokenize text using LumiOpen/Poro-34B model's tokenizer\n",
    "\n",
    "The creators of the [Poro-34B-model](https://huggingface.co/LumiOpen/Poro-34B) generated a new tokenizer for the model training. Specifically, they trained custom byte-level [BPE tokenizer](https://huggingface.co/learn/nlp-course/chapter6/5) to handle multilingual text (Finnish & English) and code efficiently. This is based on the article [\"Poro 34B and the Blessing of Multilinguality\"](https://arxiv.org/pdf/2404.01856).\n",
    "\n",
    "Let's demonstrate how text is chunked into pieces called tokens and how these tokens are represented as numerical IDs in the token vocabulary.\n",
    "\n",
    "Here’s how the process works:\n",
    "* Text input: We start with a raw input string (e.g., a sentence).\n",
    "* Tokenization: The tokenizer will break the input into smaller units called tokens.\n",
    "* Numerical IDs: These tokens are mapped to corresponding numerical IDs from the tokenizer’s vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d26464-d591-44b9-b30e-9f5da1dddf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the Poro-34B tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LumiOpen/Poro-34B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0689ee6-f5c3-4de4-8679-097d64e33881",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = \"Hello world!\"\n",
    "\n",
    "# Tokenize the text and get token IDs (numerical representation)\n",
    "token_ids1 = tokenizer(text_1)\n",
    "print(\"Token IDs:\", token_ids1['input_ids'])\n",
    "\n",
    "# Get tokens (subwords) from the text\n",
    "tokens1 = tokenizer.tokenize(text_1)\n",
    "print(\"Tokens:\", tokens1)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eb0c57-5dca-4c0a-a38d-91bfa8a3a422",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = \"Large Language Models are AI systems trained in vasts amounts of data.\"\n",
    "\n",
    "# Tokenize the text and get token IDs (numerical representation)\n",
    "token_ids2 = tokenizer(text_2)\n",
    "print(\"Token IDs:\", token_ids2['input_ids'])\n",
    "\n",
    "# Get tokens (subwords) from the text\n",
    "tokens2 = tokenizer.tokenize(text_2)\n",
    "print(\"Tokens:\", tokens2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad27ec9f-6ece-4a64-a8fb-e1ce8ff6f3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode token IDs back into human-readable form (decoded text)\n",
    "decoded_text = tokenizer.decode(token_ids2['input_ids'])\n",
    "print(\"Decoded text:\", decoded_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3119",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
