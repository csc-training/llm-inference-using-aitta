{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c3a6e18-7899-4a48-95ae-b5b7c3a5a642",
   "metadata": {},
   "source": [
    "# Tokenize text using LumiOpen/Llama-Poro-2-70B-Instruct's tokenizer\n",
    "\n",
    "[`LumiOpen/Llama-Poro-2-70B-Instruct`](https://huggingface.co/LumiOpen/Llama-Poro-2-70B-Instruct) is based on the Llama 3.1 70B architecture and has been fine-tuned for instruction following and conversational AI applications. The model supports both English and Finnish conversations. It has the same tokenizer as the original model. \n",
    "\n",
    "Let's demonstrate how text is chunked into pieces called tokens and how these tokens are represented as numerical IDs in the token vocabulary.\n",
    "\n",
    "Here’s how the process works:\n",
    "* Text input: We start with a raw input string (e.g., a sentence).\n",
    "* Tokenization: The tokenizer will break the input into smaller units called tokens.\n",
    "* Numerical IDs: These tokens are mapped to corresponding numerical IDs from the tokenizer’s vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d26464-d591-44b9-b30e-9f5da1dddf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the Llama-Poro-2-70B-Instruct-tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LumiOpen/Llama-Poro-2-70B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0689ee6-f5c3-4de4-8679-097d64e33881",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = \"Hello world!\"\n",
    "\n",
    "# Tokenize the text and get token IDs (numerical representation)\n",
    "token_ids1 = tokenizer(text_1)\n",
    "print(\"Token IDs:\", token_ids1['input_ids'])\n",
    "\n",
    "# Get tokens (subwords) from the text\n",
    "tokens1 = tokenizer.tokenize(text_1)\n",
    "print(\"Tokens:\", tokens1)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eb0c57-5dca-4c0a-a38d-91bfa8a3a422",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = \"Large Language Models are AI systems trained in vasts amounts of data.\"\n",
    "\n",
    "# Tokenize the text and get token IDs (numerical representation)\n",
    "token_ids2 = tokenizer(text_2)\n",
    "print(\"Token IDs:\", token_ids2['input_ids'])\n",
    "\n",
    "# Get tokens (subwords) from the text\n",
    "tokens2 = tokenizer.tokenize(text_2)\n",
    "print(\"Tokens:\", tokens2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad27ec9f-6ece-4a64-a8fb-e1ce8ff6f3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode token IDs back into human-readable form (decoded text)\n",
    "decoded_text = tokenizer.decode(token_ids2['input_ids'])\n",
    "print(\"Decoded text:\", decoded_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30a87ba-3019-4801-ab21-a6e2840a9715",
   "metadata": {},
   "source": [
    "# Test the older Poro-34B-chat model's tokenizer\n",
    "\n",
    "The creators of the [LumiOpen/Poro-34B-chat](https://huggingface.co/LumiOpen/Poro-34B-chat) generated a new tokenizer for the base model training. Specifically, they trained custom byte-level [BPE tokenizer](https://huggingface.co/learn/nlp-course/chapter6/5) to handle multilingual text (Finnish & English) and code efficiently. This is based on the article [\"Poro 34B and the Blessing of Multilinguality\"](https://arxiv.org/pdf/2404.01856)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f02271-a921-46e7-bb64-2896853e8779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Poro-34B-chat tokenizer\n",
    "tokenizer_older = AutoTokenizer.from_pretrained(\"LumiOpen/Poro-34B-chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490a2a4b-801a-4896-9dd1-f4be4e5d3c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_3 = \"Hello world!\"\n",
    "\n",
    "# Tokenize the text and get token IDs (numerical representation)\n",
    "token_ids3 = tokenizer_older(text_3)\n",
    "print(\"Token IDs:\", token_ids3['input_ids'])\n",
    "\n",
    "# Get tokens (subwords) from the text\n",
    "tokens3 = tokenizer_older.tokenize(text_3)\n",
    "print(\"Tokens:\", tokens3)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137fe638-c4a7-447e-98da-50e3181366dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_4 = \"Large Language Models are AI systems trained in vasts amounts of data.\"\n",
    "\n",
    "# Tokenize the text and get token IDs (numerical representation)\n",
    "token_ids4 = tokenizer_older(text_4)\n",
    "print(\"Token IDs:\", token_ids4['input_ids'])\n",
    "\n",
    "# Get tokens (subwords) from the text\n",
    "tokens4 = tokenizer_older.tokenize(text_4)\n",
    "print(\"Tokens:\", tokens4)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68041901-4322-437a-b43e-97e3556d838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode token IDs back into human-readable form (decoded text)\n",
    "decoded_text = tokenizer_older.decode(token_ids4['input_ids'])\n",
    "print(\"Decoded text:\", decoded_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765937dd-5265-4b0b-94dc-b71696943cc4",
   "metadata": {},
   "source": [
    "**See what happens if you use the wrong tokenizer to decode text...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2756ef27-e1a2-44eb-b6a9-6e10b457f289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the Llama-Poro-2-70B-Instruct tokenizer \n",
    "decoded_text = tokenizer.decode(token_ids4['input_ids']) # these input id's came from the Poro-34B-chat models tokenizer\n",
    "print(\"Decoded text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd58148b-a371-4e5f-bf0a-280d528c716b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the Poro-34B-chat tokenizer \n",
    "decoded_text = tokenizer_older.decode(token_ids2['input_ids']) # these input id's came from the Llama-Poro-2-70B-Instruct models tokenizer\n",
    "print(\"Decoded text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f881641-07aa-4d81-8aab-2a2f0252d660",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
