{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10d8dc8e-a2ed-45f5-a17f-0e354f73b166",
   "metadata": {},
   "source": [
    "## Prompt testing\n",
    "\n",
    "Next, we will explore the capabilities of a chat-tuned LLM using different prompting techniques.\n",
    "Choose a model optimized for conversational tasks. You can choose another chat-tuned model from Aitta's model catalogue. You don’t need to manually enter the model’s name—we will retrieve it using the `get_model_list()` method and store it in the variable `model_name`.\n",
    "\n",
    "**Remember basic guidelines for creating efficient prompts:**\n",
    "\n",
    "* Be specific\n",
    "* Provide context\n",
    "* Define output format\n",
    "* Use role-based prompts\n",
    "\n",
    "First, we need to use the API key to configure the Python client to establish a connection to the API endpoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8577a8f5-e59f-41c5-837f-4a04a3fd4bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"<API-KEY>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ebc4a3b-07d1-410f-a1d2-de11c5a0be07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aitta_client import Model, Client, StaticAccessTokenSource\n",
    "import openai\n",
    "\n",
    "token_source = StaticAccessTokenSource(api_key)\n",
    "aitta_client = Client(\"https://api-staging-aitta.2.rahtiapp.fi\", token_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79ad1b3-ceb7-4927-bf49-b34342e8d6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the list of models (contains only one item, since API keys are model specific)\n",
    "models = aitta_client.get_model_list()\n",
    "#print(type(models))\n",
    "\n",
    "# Extract the single model object\n",
    "model = models[0]\n",
    "#print(type(model))\n",
    "\n",
    "# Get the model ID/name as a string to be used in the model loading phase in the next cell\n",
    "model_name = model.id\n",
    "\n",
    "# Print the model name\n",
    "print(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec554ae-4cb1-47e4-9e96-c225545a7dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = Model.load(model_name, aitta_client)\n",
    "print(model.description)\n",
    "\n",
    "# configure OpenAI client to use the Aitta OpenAI compatibility endpoints\n",
    "client = openai.OpenAI(api_key=token_source.get_access_token(), base_url=model.openai_api_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79fc8c68-84b1-4d36-a1d4-838f230833d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get responses\n",
    "def get_response(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        model=model.id,\n",
    "        max_completion_tokens=200,\n",
    "        stream=False  # response streaming is currently not supported by Aitta\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dd5567-caa8-42b9-8c46-94de88c934a9",
   "metadata": {},
   "source": [
    "## Examples of refining prompts\n",
    "\n",
    "Below are examples demonstrating how to improve prompts for better AI responses. Each case starts with a weaker prompt, followed by an improved version that provides clearer instructions, context, or structure.\n",
    "\n",
    "Feel free to modify these examples, create your own, and experiment with different models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb5cfe3-ce96-451d-a3bc-94f3651adc40",
   "metadata": {},
   "source": [
    "### Example 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d091ba9d-1cc1-4ca4-b8e9-14557dd8e36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Explain machine learning.\n",
    "\"\"\"\n",
    "\n",
    "response = get_response(prompt)\n",
    "print(f\"Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b62495-67cb-42e7-bc19-0bbcadd9a673",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are a university professor explaining machine learning to a beginner.\n",
    "Explain machine learning.\n",
    "Keep the explanation short, only a few sentences, and don't be too descriptive.\n",
    "\"\"\"\n",
    "\n",
    "response = get_response(prompt)\n",
    "print(f\"Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a024e8ac-6537-451a-a4c4-e25f5533f2c6",
   "metadata": {},
   "source": [
    "### Example 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22eac28-d463-4a04-8333-b2fbb9af414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Explain quantum computing.\n",
    "\"\"\"\n",
    "\n",
    "response = get_response(prompt)\n",
    "print(f\"Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7c878b-9d3d-4560-894c-a1edb4ffd6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Explain quantum computing in two sentences, using simple language.\n",
    "Explain like I would be five years old.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "response = get_response(prompt)\n",
    "print(f\"Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a190cb-70fd-47d8-b39a-94b94c544361",
   "metadata": {},
   "source": [
    "## Example 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2523d89-5322-45a6-aa56-70c4b007e626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.lumi-supercomputer.eu/open-euro-llm/\n",
    "text = \"\"\"\n",
    "A consortium of 20 leading European research institutions, companies and EuroHPC centres coordinated by Jan Hajič (Charles University, Czechia) and co-led by Peter Sarlin (AMD Silo AI, Finland) will build a family of performant, multilingual, large language foundation models for commercial, industrial and public services. \n",
    "LUMI will be one of the platforms used in the project. The transparent and compliant open-source models will democratize access to high-quality AI technologies and strengthen the ability of European companies to compete on a global market and public organizations to produce impactful public services.\n",
    "The OpenEuroLLM project is aligned with the imperative to improve Europe’s competitiveness and digital sovereignty. The project is a prime example of the type of technology infrastructure needed to lower thresholds for European AI product development and refinement, demonstrating the strength of transparency, openness and community involvement, \n",
    "values largely recognized across the European tech ecosystem. The models will be developed within Europe’s robust regulatory framework, ensuring alignment with European values while maintaining technological excellence.\n",
    "Cooperating with open-source and open science communities like LAION, open-sci and OpenML, and additional experts in the field assembled in the project’s Open Strategic Partnership Board, OpenEuroLLM will ensure that the models, software, data and evaluation will be fully open and can be fine-tuned and instruction-tuned for specific industry and \n",
    "public sector needs. These performant multilingual models preserve both linguistic and cultural diversity, enabling European companies to develop high-quality products and services in the era of AI.\n",
    "The project, which has been awarded the STEP (Strategic Technologies for Europe Platform) seal, leverages support from previous European projects and the experience of the partners and their results, including large repositories of high-quality data and pilot LLMs developed previously. The consortium commences its work on February 1st, 2025, with funding from the European Commission under the Digital Europe Programme.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"Tell me about this project:\\n{text}\"\n",
    "\n",
    "repsonse = get_response(prompt)\n",
    "print(f\"Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126754d6-1249-4688-ae61-a8a64d6d5eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "better_prompt = f\"\"\"Analyze the OpenEuroLLM project described in the following text and provide\n",
    "A concise summary of the project's main goal with 3-5 bulletpoints.\n",
    "Text: {text}\n",
    "\"\"\"\n",
    "\n",
    "response = get_response(better_prompt)\n",
    "print(f\"Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb04096-dfea-46f8-bd55-0c1a80078459",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79b6800",
   "metadata": {},
   "source": [
    "## Experiment with different model and prompt\n",
    "\n",
    "Now that you have completed the examples, you can modify the model or the prompt to observe how these changes affect the output. Feel free to experiment with different configurations and see how the model's responses vary. This is a great way to understand the impact of prompt engineering and model adjustments on the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc290780",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOU CAN CODE HERE ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3119",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
