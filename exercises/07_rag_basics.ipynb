{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fd15c69-324b-4f4a-b92b-a5ec9699e6cc",
   "metadata": {},
   "source": [
    "# Simple Retrieval Augmented Generation (RAG) Example\n",
    "\n",
    "## What is RAG?\n",
    "Retrieval Augmented Generation (RAG) is a technique that **enhances LLM responses** by retrieving relevant context from an external knowledge source before generating an answer. Instead of relying only on the model's training data, **RAG fetches real-time information** to improve accuracy.\n",
    "\n",
    "## Data flow in RAG applications\n",
    "\n",
    "In RAG, data indexing is the process of organizing and storing large datasets in a structured way, making it easier to quickly search and retrieve relevant information. Retrieval involves querying the indexed data to find the most relevant documents or pieces of information for a given input or question. Generation refers to using the retrieved data to generate a coherent and contextually appropriate response, leveraging the information from the documents to enhance the accuracy and relevance of the output.\n",
    "\n",
    "![](./images/rag-pipeline-v2.png)\n",
    "\n",
    "## Steps in this exercise\n",
    "\n",
    "1. **Chunk the text using embedding model**: We begin by loading a sample document to use as our knowledge source. We'll then split the document into smaller, manageable pieces (chunks) based on the embedding model's context length. This is crucial because LLMs often have token limits, and chunking ensures that each piece of text can be processed effectively.\n",
    "\n",
    "2. **Create embeddings and store in a vector database**: Next, we convert each chunk into a vector representation using a sentence embedding model. The resulting vectors capture the meaning of each text chunk. We then store these embeddings in a simple list that acts as our vector database.\n",
    "\n",
    "3. **Cosine similarity - find relevant text chunks**: To retrieve the most relevant text, we encode the query using the same embedding model and then calculate the cosine similarity between the query's vector and the vectors of the text chunks. The cosine similarity score helps us determine which text chunks are most similar to the query. The higher the score, the more relevant the chunk is.\n",
    "\n",
    "4. **Set up LLM and gerenate response with RAG**: Finally, we pass the query and the retrieved context to an LLM (Large Language Model). The LLM will use the retrieved context to generate a more informed, accurate response. By combining retrieval with generation, we ensure the model has access to up-to-date or domain-specific information that might not have been part of its training data.\n",
    "\n",
    "-------\n",
    "\n",
    "This exercise a simplified implementation of RAG, designed to help you understand the core logic behind the process. There are various ways to optimize and scale this process in real-world applications.\n",
    "\n",
    "At the moment, it is not possible to create embeddings using the Python client provided by Aitta. However, we will use the Sentence Transformers library for creating embeddings. LLM usage is possible as in other exercises.\n",
    "\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609f1b75-c88d-42e2-8ce6-6a98eb88c61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file \"ai_factories.txt\" in read mode to be used\n",
    "with open(\"ai_factories.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[:500])  # Print first 500 characters to check the content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ddc408-4a11-4380-8138-8cdfb848cb70",
   "metadata": {},
   "source": [
    "## Choosing an embedding model  \n",
    "\n",
    "For this example, we are using [**`all-MiniLM-L6-v2`**](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2), a small and efficient sentence transformer model.  \n",
    "- It is lightweight and can run on CPUs, making it suitable for our current environment.  \n",
    "- In the future, we could generate embeddings using **Aitta API**, which allows us to leverage GPU acceleration for faster processing.  \n",
    "\n",
    "### Check model's `max_sequence_length` & `embeddings_dimension`?  \n",
    "Before chunking text, we need to understand the model’s constraints:  \n",
    "1. **Max sequence length** (also known as **context length** or **max input tokens** – The maximum number of tokens the model can process in a single pass.  \n",
    "2. **Embedding dimension** – The size of the vector (numerical representation) generated by the model for each input sentence.\n",
    "\n",
    "Below, we initialize the model and check these properties.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d21c5d-f679-4320-a01f-f9dec91771b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the embedding model\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Print model properties\n",
    "print(\"Max Sequence Length:\", embedding_model.max_seq_length)  # Context length\n",
    "print(\"Embedding Dimension:\", embedding_model.get_sentence_embedding_dimension())  # Vector size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbea824-7762-4a3c-9b9d-8b1ad1005c78",
   "metadata": {},
   "source": [
    "## Chunking the text  \n",
    "\n",
    "Since our embedding model has a **maximum sequence length**, we need to split the document into smaller, manageable chunks. This ensures that:  \n",
    "- Each chunk fits within the **model’s token limit** for accurate embedding.  \n",
    "- Sentences remain intact by **splitting at logical points** (e.g., periods, question marks).  \n",
    "\n",
    "### Setting up the text splitter  \n",
    "We use `RecursiveCharacterTextSplitter` from [**LangChain**](https://github.com/langchain-ai/langchain), with these parameters:  \n",
    "- **`chunk_size`** → Defines the maximum number of tokens per chunk.  \n",
    "- **`chunk_overlap`** → Ensures that chunks share overlapping content to maintain continuity (if set to a value greater than 0)   \n",
    "- **`separators`** → Defines where the text is split (e.g., at new paragraphs or punctuation).\n",
    "- **`length_function`** → Uses a tokenizer to count tokens instead of characters for accurate chunking. Without this, splitting is based on character count, which may lead to incorrect chunk sizes.\n",
    "\n",
    "Next, we **split the document into chunks** and verify their correctness.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff1631a-0865-43fd-9741-af0689fee9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer to count tokens accurately\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Function to count tokens in a text chunk\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.encode(text, add_special_tokens=False))\n",
    "\n",
    "# Setting up the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50,  # Limit each chunk to 50 tokens (small for demonstration)\n",
    "    chunk_overlap=0,  # No overlap between chunks in this case\n",
    "    length_function=count_tokens,  # Use token-based counting instead of character count\n",
    "    separators=[\"\\n\\n\", \". \", \"? \", \"! \"]  # Prioritize splitting at paragraph or sentence boundaries\n",
    ")\n",
    "\n",
    "# Split the document into smaller chunks\n",
    "chunks = text_splitter.split_text(text)\n",
    "\n",
    "#  Display chunk statistics and example\n",
    "print(f\"Total chunks created: {len(chunks)}\")  # Show total number of chunks\n",
    "print(f\"Sample chunk:\\n{chunks[0]}\")  # Print the first chunk as an example\n",
    "print(f\"Tokens in first chunk: {count_tokens(chunks[0])}\")  # Verify token count in the first chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395cd9c1-f3b0-4931-8085-1d3dfbfc4d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to see tokens in the first chunk\n",
    "# tokenizer.tokenize(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56841281-2648-4b86-bc51-adec75da3278",
   "metadata": {},
   "source": [
    "## Initializing a simple vector database\n",
    "\n",
    "In this example, we use a **simple vector database** (a list stored in memory) to store the text chunks and their corresponding embeddings. Each text chunk is transformed into a vector representation using the embedding model. The initialized `VECTOR_DB` is a list of dictionaries, where each entry contains a **text chunk** and its corresponding **embedding vector**, enabling efficient retrieval and comparison of text data for the RAG process.\n",
    "\n",
    "However, in real-world applications, it's typically more efficient to use dedicated vector databases or indexing libraries like **FAISS**, which are optimized for storing and retrieving high-dimensional vectors. These libraries support fast similarity search, making them essential for handling large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9272ec7e-17f2-45bb-ba82-c737d68edba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a simple vector database as a list\n",
    "VECTOR_DB = []\n",
    "\n",
    "# Iterate through the chunks of text, generate embeddings, and store them in the VECTOR_DB\n",
    "for chunk in chunks:\n",
    "    embedding = embedding_model.encode(chunk)  # Convert the text chunk into a vector representation\n",
    "    \n",
    "    # Append the chunk and its corresponding embedding to the vector database\n",
    "    VECTOR_DB.append({\"text\": chunk, \"embedding\": embedding})\n",
    "\n",
    "print(f\"Stored {len(VECTOR_DB)} chunks in VECTOR_DB.\")  # Display the number of stored chunks\n",
    "\n",
    "# Display a sample entry\n",
    "sample_entry = VECTOR_DB[0]\n",
    "embedding_sample = sample_entry[\"embedding\"][:5]\n",
    "text_sample = sample_entry[\"text\"]\n",
    "\n",
    "print(f\"Sample entry:\\nText: {text_sample}...\")\n",
    "print(f\"Embedding (first 5 values): {embedding_sample}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0398ede-a648-4e35-b138-b5fd790df2f9",
   "metadata": {},
   "source": [
    "## Find relevant text chunks using cosine similarity\n",
    "\n",
    "To perform retrieval, we need to embed a query and then compare it to the stored vectors in the vector database. The goal is to find the most relevant text chunks based on the similarity between the query's embedding and the embeddings stored in the vector database.\n",
    "\n",
    "We will use **Cosine Similarity** to measure how similar the query's embedding is to each of the stored embeddings. \n",
    "\n",
    "**Cosine Similarity** measures the cosine of the angle between two non-zero vectors, which is a measure of similarity between them. The formula is:\n",
    "\n",
    "$$\n",
    "\\text{cosine\\_similarity} = \\frac{{A \\cdot B}}{{\\|A\\| \\|B\\|}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- A and B are the vectors to be compared.\n",
    "- The **dot product** A dot B is calculated by multiplying corresponding components of the vectors and summing the results.\n",
    "- The result is then divided by the product of the magnitudes of the vectors, A and B.\n",
    "\n",
    "\n",
    "Cosine similarity ranges from -1 to 1:\n",
    "- **1** means that the two vectors are identical (maximum similarity).\n",
    "- **0** means that the two vectors are orthogonal (no similarity).\n",
    "- **-1** means that the two vectors are diametrically opposite (maximum dissimilarity).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98797874-3f75-4671-bf9d-5c9ca731a98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/blog/ngxson/make-your-own-rag\n",
    "def cosine_similarity(a, b):\n",
    "  dot_product = sum([x * y for x, y in zip(a, b)])\n",
    "  norm_a = sum([x ** 2 for x in a]) ** 0.5\n",
    "  norm_b = sum([x ** 2 for x in b]) ** 0.5\n",
    "  return dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb4c1a7-3fd0-4169-b2ff-927dcb7f77fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query text \n",
    "query_text = \"What is the goal of AI factories in Europe?\"\n",
    "\n",
    "# Generate the embedding for the query\n",
    "query_embedding = embedding_model.encode(query_text)\n",
    "\n",
    "# Now, calculate the cosine similarity between the query embedding and each embedding in the VECTOR_DB\n",
    "similarities = []\n",
    "\n",
    "# Iterate over VECTOR_DB to calculate similarity with each stored chunk\n",
    "for entry in VECTOR_DB:\n",
    "    stored_embedding = entry[\"embedding\"]\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity = cosine_similarity(query_embedding, stored_embedding)\n",
    "    similarities.append((entry[\"text\"], similarity))\n",
    "\n",
    "# Sort the results by similarity in descending order and show the most similar entries\n",
    "similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "# Display the top 3 most similar chunks\n",
    "print(f\"Text and it's similarity to test query:\\n\")\n",
    "for i in range(len(similarities)):\n",
    "    print(f\"Similarity: {similarities[i][1]:.4f}\")\n",
    "    print(f\"Text: {similarities[i][0][:200]}...\")  # Display only first 200 characters of the text\n",
    "    print(\"#######\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b0c14c-a2e6-4208-9603-b9b8f75e1c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to check datatype of embeddings\n",
    "#type(VECTOR_DB[0]['embedding'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec38b4a-52c0-4e00-82a2-8ee375ab4dcb",
   "metadata": {},
   "source": [
    "## Using RAG with chat-model\n",
    "\n",
    "Let's combine it all together to generate responses!\n",
    "\n",
    "In this section, we are going to use the **RAG** technique to generate a meaningful and contextually aware answer to a user's query. The process involves combining the power of **embedding-based vector search** with a chat-model [`LumiOpen/Poro-34B-chat`](https://huggingface.co/LumiOpen/Poro-34B-chat) to provide an informed response. We use a previously generated vector database.\n",
    "\n",
    "### Setting up the API client and model to use RAG\n",
    "\n",
    "We will configure the API client and load the LLM that will be used for generating responses within the RAG framework.\n",
    "\n",
    "**API Key Configuration and model loading**  \n",
    "   First, we configure the **Aitta-client** with an access token and API URL to authenticate and interact with the AI model. This setup allows us to securely communicate with the model and load the required resources.\n",
    "\n",
    "**OpenAI compatibility**  \n",
    "   Next, we set up the **OpenAI client** to point to the AI model’s OpenAI-compatible API endpoint. This enables us to send queries and receive responses through the OpenAI-compatible interface provided by Aitta.\n",
    "\n",
    "After completing these setup steps, the model is ready to be used. We need to embedd the query, retrieve relevant text chunks and generate response using instuction prompt together with query and retrieved texts. We can then  send queries along with the retrieved context to generate informed responses based on the retrieved knowledge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3661aee-9e9e-4d7e-b7de-3e9471e32fa5",
   "metadata": {},
   "source": [
    "## Using RAG with chat-model\n",
    "\n",
    "Let's combine it all together to generate responses!\n",
    "\n",
    "In this section, we are going to use the **RAG** technique to generate a meaningful and contextually aware answer to a user's query. The process involves combining the power of **embedding-based vector search** with a chat-model [`LumiOpen/Poro-34B-chat`](https://huggingface.co/LumiOpen/Poro-34B-chat) to provide an informed response. We use a previously generated vector database.\n",
    "\n",
    "### Setting up the API client and model to use RAG\n",
    "\n",
    "We will configure the API client and load the LLM that will be used for generating responses within the RAG framework.\n",
    "\n",
    "#### **API Key Configuration and model loading**  \n",
    "   First, we configure the **Aitta-client** with an access token and API URL to authenticate and interact with the AI model. This setup ensures secure communication between the client and the model, allowing us to load the necessary resources for generating responses.\n",
    "\n",
    "#### **OpenAI compatibility**  \n",
    "Next, we set up the **OpenAI client**. This step enables us to send queries and receive responses through an OpenAI-compatibility provided by Aitta.\n",
    "\n",
    "After completing these setup steps, the model is ready for use. We will then:\n",
    "- **Embed the query**: Convert the user's query into a vector representation.\n",
    "- **Retrieve relevant text chunks**: Search the vector database for the most relevant chunks based on the query’s embedding.\n",
    "- **Generate a response**: Use the retrieved context, the query, and an instruction prompt to guide the model’s response generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fc4c92-2fd9-4988-b345-411f7a6d1e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"<API-KEY>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c91cbe-a142-48f0-bb43-fa7aa5d4cc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from aitta_client import Model, Client, StaticAccessTokenSource\n",
    "\n",
    "# configure Client instance with API URL and access token\n",
    "token_source = StaticAccessTokenSource(api_key)\n",
    "aitta_client = Client(\"https://api-staging-aitta.2.rahtiapp.fi\", token_source)\n",
    "\n",
    "# load the LumiOpen/Poro-34B-chat model\n",
    "poro_model = Model.load(\"LumiOpen/Poro-34B-chat\", aitta_client)\n",
    "print(poro_model.description)\n",
    "\n",
    "# configure OpenAI client to use the Aitta OpenAI compatibility endpoints\n",
    "client = openai.OpenAI(api_key=token_source.get_access_token(), base_url=poro_model.openai_api_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0836549e-f2a3-4930-b18c-0ada537c758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/blog/ngxson/make-your-own-rag\n",
    "def cosine_similarity(a, b):\n",
    "  dot_product = sum([x * y for x, y in zip(a, b)])\n",
    "  norm_a = sum([x ** 2 for x in a]) ** 0.5\n",
    "  norm_b = sum([x ** 2 for x in b]) ** 0.5\n",
    "  return dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3896e1-b53d-478f-8ace-7d34824c2647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve top N most relevant chunks using cosine similarity\n",
    "def retrieve(query, top_n=2):\n",
    "    query_embedding = embedding_model.encode(query)\n",
    "    #print(type(query_embedding))\n",
    "    \n",
    "    similarities = []\n",
    "    for entry in VECTOR_DB:\n",
    "\n",
    "        chunk = entry[\"text\"]\n",
    "        embedding = entry[\"embedding\"] \n",
    "     \n",
    "        similarity = cosine_similarity(query_embedding, embedding) \n",
    "        similarities.append((chunk, similarity))\n",
    "        \n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f9f63b-9ec7-43a2-92f4-8ad224f448a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_query = \"What are AI factories?\"\n",
    "retrieved_knowledge = retrieve(input_query)\n",
    "\n",
    "# UNCOMMENT lines below to see retrieved text chunks\n",
    "#print('Retrieved knowledge:')\n",
    "#for chunk, similarity in retrieved_knowledge:\n",
    "#  print(f' - (similarity: {similarity:.2f}) {chunk}')\n",
    "\n",
    "# Format the context for the prompt\n",
    "instruction_prompt = \"\"\"\n",
    "You are a helpful chatbot.\n",
    "Use only the following pieces of context to answer the question. Don't make up any new information:\n",
    "\"\"\"\n",
    "# Add the chunks to instruction prompt\n",
    "instruction_prompt += '\\n'.join([f' - {chunk}' for chunk, similarity in retrieved_knowledge])\n",
    "\n",
    "print(f\"Instruction prompt: {instruction_prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9324473-99ba-498b-8f83-a564a53b868e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Call the OpenAI API to generate the response using the retrieved context\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": instruction_prompt + input_query\n",
    "        }\n",
    "    ],\n",
    "    model=poro_model.id,\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "# Display the answer\n",
    "answer = response.choices[0].message.content\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a71469-efa7-46d3-a812-9471029c9284",
   "metadata": {},
   "source": [
    "**Finally** we can test how the model would have answered the query without using external data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c394cb48-fba6-45a4-b11e-c10bd9881aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the OpenAI API to generate the response using the retrieved context\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": input_query #Use ONLY input query\n",
    "        }\n",
    "    ],\n",
    "    model=poro_model.id,\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "# Display the answer\n",
    "answer = response.choices[0].message.content\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365bae49",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we covered the foundational steps of RAG. In real-world use cases, it's essential to use optimized vector databases for efficient search and retrieval. This removes the need for manually coding search functions, like the cosine similarity example demonstrated earlier, where we computed similarities between vectors manually.\n",
    "\n",
    "In contrast, using optimized libraries like FAISS abstracts away much of this complexity, allowing for faster and more scalable retrieval. In the next exercise, we’ll continue with a simple example, introducing FAISS to show how it enhances vector search and retrieval performance.\n",
    "\n",
    "Next exercise: [08_simpleRAG-using-FAISS.ipynb](./08_simpleRAG-using-FAISS.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecc15c9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
