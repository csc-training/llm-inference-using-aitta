{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96212a81-6119-490a-8e93-3e6c571ee452",
   "metadata": {},
   "source": [
    "# Simple RAG example using Facebook AI Similarity Search (FAISS)\n",
    "\n",
    "In this example, we'll demonstrate how to use **FAISS** for similarity-based document retrieval. We will simulate a small **mock dataset** of fictional documents and use **Sentence Transformers** to encode them into vectors. We will then build an **FAISS index** to enable fast and efficient similarity search. Finally, we will simulate a **RAG** system where we retrieve the most relevant documents and use them to generate an answer.\n",
    "\n",
    "\n",
    "## Steps Overview:\n",
    "1. **Create a mock dataset**: We create a small set of fictional documents.\n",
    "2. **Generate embeddings**: We use the **SentenceTransformer** model to convert these documents into vector embeddings.\n",
    "3. **Build FAISS index**: We build a **FAISS index** that will store these vector embeddings for fast similarity search.\n",
    "4. **Search and retrieve**: We perform a similarity search based on a query and retrieve the most relevant document(s).\n",
    "5. **Answer generation**: Using the retrieved document(s), we simulate a **RAG pipeline** to generate a response using OpenAI compatible API Aitta provides.\n",
    "\n",
    "This example demonstrates how **FAISS** can be used for efficient document retrieval, and how **RAG** can help generate contextually relevant answers from these documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea9bdb5b-09e6-4070-888e-27a3bb25ba84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai==1.66.5 (from -r requirements.txt (line 1))\n",
      "  Downloading openai-1.66.5-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting langchain (from -r requirements.txt (line 2))\n",
      "  Downloading langchain-0.3.21-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting sentence-transformers (from -r requirements.txt (line 3))\n",
      "  Downloading sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (4.44.0)\n",
      "Collecting aitta-client (from -r requirements.txt (line 5))\n",
      "  Downloading aitta_client-0.2.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting tf-keras (from -r requirements.txt (line 6))\n",
      "  Downloading tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting faiss-cpu (from -r requirements.txt (line 7))\n",
      "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from openai==1.66.5->-r requirements.txt (line 1)) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from openai==1.66.5->-r requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from openai==1.66.5->-r requirements.txt (line 1)) (0.27.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai==1.66.5->-r requirements.txt (line 1))\n",
      "  Downloading jiter-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai==1.66.5->-r requirements.txt (line 1))\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from openai==1.66.5->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.11/site-packages (from openai==1.66.5->-r requirements.txt (line 1)) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/conda/lib/python3.11/site-packages (from openai==1.66.5->-r requirements.txt (line 1)) (4.11.0)\n",
      "Collecting langchain-core<1.0.0,>=0.3.45 (from langchain->-r requirements.txt (line 2))\n",
      "  Downloading langchain_core-0.3.48-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.7 (from langchain->-r requirements.txt (line 2))\n",
      "  Downloading langchain_text_splitters-0.3.7-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain->-r requirements.txt (line 2))\n",
      "  Downloading langsmith-0.3.18-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.11/site-packages (from langchain->-r requirements.txt (line 2)) (2.0.30)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.11/site-packages (from langchain->-r requirements.txt (line 2)) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.11/site-packages (from langchain->-r requirements.txt (line 2)) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from sentence-transformers->-r requirements.txt (line 3)) (2.3.1+cpu)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (from sentence-transformers->-r requirements.txt (line 3)) (1.5.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (from sentence-transformers->-r requirements.txt (line 3)) (1.14.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.11/site-packages (from sentence-transformers->-r requirements.txt (line 3)) (0.24.6)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.11/site-packages (from sentence-transformers->-r requirements.txt (line 3)) (10.4.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers->-r requirements.txt (line 4)) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers->-r requirements.txt (line 4)) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers->-r requirements.txt (line 4)) (24.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers->-r requirements.txt (line 4)) (2024.7.24)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers->-r requirements.txt (line 4)) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.11/site-packages (from transformers->-r requirements.txt (line 4)) (0.19.1)\n",
      "INFO: pip is looking at multiple versions of aitta-client to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain (from -r requirements.txt (line 2))\n",
      "  Downloading langchain-0.3.20-py3-none-any.whl.metadata (7.7 kB)\n",
      "  Downloading langchain-0.3.19-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.11/site-packages (from langchain->-r requirements.txt (line 2)) (3.9.5)\n",
      "Collecting tenacity!=8.4.0,<10,>=8.1.0 (from langchain->-r requirements.txt (line 2))\n",
      "  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting langchain (from -r requirements.txt (line 2))\n",
      "  Downloading langchain-0.3.18-py3-none-any.whl.metadata (7.8 kB)\n",
      "  Downloading langchain-0.3.17-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.16-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.15-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.14-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langsmith<0.3,>=0.1.17 (from langchain->-r requirements.txt (line 2))\n",
      "  Downloading langsmith-0.2.11-py3-none-any.whl.metadata (14 kB)\n",
      "INFO: pip is still looking at multiple versions of aitta-client to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain (from -r requirements.txt (line 2))\n",
      "  Downloading langchain-0.3.13-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.12-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.11-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.10-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain->-r requirements.txt (line 2))\n",
      "  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting langchain (from -r requirements.txt (line 2))\n",
      "  Downloading langchain-0.3.9-py3-none-any.whl.metadata (7.1 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading langchain-0.3.8-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.7-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.6-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.5-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.4-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain->-r requirements.txt (line 2))\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting langchain (from -r requirements.txt (line 2))\n",
      "  Downloading langchain-0.3.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.2.17-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.43 (from langchain->-r requirements.txt (line 2))\n",
      "  Downloading langchain_core-0.2.43-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain->-r requirements.txt (line 2))\n",
      "  Downloading langchain_text_splitters-0.2.4-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai==1.66.5->-r requirements.txt (line 1))\n",
      "  Downloading pydantic-2.5.3-py3-none-any.whl.metadata (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyJWT~=2.8.0 in /opt/conda/lib/python3.11/site-packages (from aitta-client->-r requirements.txt (line 5)) (2.8.0)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1.9.0->openai==1.66.5->-r requirements.txt (line 1))\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.14.6 (from pydantic<3,>=1.9.0->openai==1.66.5->-r requirements.txt (line 1))\n",
      "  Downloading pydantic_core-2.14.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting tensorflow<2.20,>=2.19 (from tf-keras->-r requirements.txt (line 6))\n",
      "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai==1.66.5->-r requirements.txt (line 1)) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai==1.66.5->-r requirements.txt (line 1)) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai==1.66.5->-r requirements.txt (line 1)) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.66.5->-r requirements.txt (line 1)) (0.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers->-r requirements.txt (line 3)) (2024.6.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.43->langchain->-r requirements.txt (line 2)) (1.33)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 2))\n",
      "  Downloading orjson-3.10.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 2))\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain->-r requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain->-r requirements.txt (line 2)) (2.2.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain->-r requirements.txt (line 2)) (3.0.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras->-r requirements.txt (line 6)) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras->-r requirements.txt (line 6)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras->-r requirements.txt (line 6)) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras->-r requirements.txt (line 6)) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras->-r requirements.txt (line 6)) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras->-r requirements.txt (line 6)) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras->-r requirements.txt (line 6)) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras->-r requirements.txt (line 6)) (4.25.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras->-r requirements.txt (line 6)) (70.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras->-r requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras->-r requirements.txt (line 6)) (2.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras->-r requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras->-r requirements.txt (line 6)) (1.64.1)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow<2.20,>=2.19->tf-keras->-r requirements.txt (line 6))\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow<2.20,>=2.19->tf-keras->-r requirements.txt (line 6))\n",
      "  Downloading keras-3.9.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras->-r requirements.txt (line 6)) (3.11.0)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow<2.20,>=2.19->tf-keras->-r requirements.txt (line 6))\n",
      "  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras->-r requirements.txt (line 6)) (0.37.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 3)) (1.12.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 3)) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 3)) (3.1.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 3)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 3)) (3.5.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras->-r requirements.txt (line 6)) (0.43.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.43->langchain->-r requirements.txt (line 2)) (2.4)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras->-r requirements.txt (line 6)) (13.7.1)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras->-r requirements.txt (line 6)) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras->-r requirements.txt (line 6)) (0.11.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras->-r requirements.txt (line 6)) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras->-r requirements.txt (line 6)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras->-r requirements.txt (line 6)) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers->-r requirements.txt (line 3)) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras->-r requirements.txt (line 6)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras->-r requirements.txt (line 6)) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras->-r requirements.txt (line 6)) (0.1.2)\n",
      "Downloading openai-1.66.5-py3-none-any.whl (571 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.1/571.1 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain-0.2.17-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aitta_client-0.2.0-py3-none-any.whl (13 kB)\n",
      "Downloading pydantic-2.5.3-py3-none-any.whl (381 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.14.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.9/275.9 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (351 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m351.8/351.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.2.43-py3-none-any.whl (397 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.1/397.1 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.2.4-py3-none-any.whl (25 kB)\n",
      "Downloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading keras-3.9.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.10.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.3/130.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tenacity, pydantic-core, orjson, ml-dtypes, jiter, faiss-cpu, annotated-types, tensorboard, requests-toolbelt, pydantic, openai, langsmith, keras, aitta-client, tensorflow, langchain-core, tf-keras, sentence-transformers, langchain-text-splitters, langchain\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml-dtypes 0.3.2\n",
      "    Uninstalling ml-dtypes-0.3.2:\n",
      "      Successfully uninstalled ml-dtypes-0.3.2\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.16.2\n",
      "    Uninstalling tensorboard-2.16.2:\n",
      "      Successfully uninstalled tensorboard-2.16.2\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.4.1\n",
      "    Uninstalling keras-3.4.1:\n",
      "      Successfully uninstalled keras-3.4.1\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.16.2\n",
      "    Uninstalling tensorflow-2.16.2:\n",
      "      Successfully uninstalled tensorflow-2.16.2\n",
      "Successfully installed aitta-client-0.2.0 annotated-types-0.7.0 faiss-cpu-1.10.0 jiter-0.9.0 keras-3.9.0 langchain-0.2.17 langchain-core-0.2.43 langchain-text-splitters-0.2.4 langsmith-0.1.147 ml-dtypes-0.5.1 openai-1.66.5 orjson-3.10.15 pydantic-2.5.3 pydantic-core-2.14.6 requests-toolbelt-1.0.0 sentence-transformers-3.4.1 tenacity-8.5.0 tensorboard-2.19.0 tensorflow-2.19.0 tf-keras-2.19.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install sentence-transformers faiss-cpu openai aitta-client\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f62c690-e7f0-4686-859c-c270448e47de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 15:38:30.151089: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-24 15:38:30.153515: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-24 15:38:30.156769: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-24 15:38:30.166001: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742830710.192803    1168 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742830710.197400    1168 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742830710.210230    1168 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742830710.210247    1168 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742830710.210249    1168 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742830710.210251    1168 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-24 15:38:30.214791: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import openai\n",
    "import openai\n",
    "\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from aitta_client import Model, Client, StaticAccessTokenSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0045d3e-dc0f-4a32-9eff-615a6e6c9e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b9617f8-f840-48d3-96b2-d5c7ce2416bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poro 34B chat is a chat-tuned version of Poro 34B trained to follow instructions in both Finnish and English.\n",
      "\n",
      "Poro was created in a collaboration between SiloGen from Silo AI, the TurkuNLP group of the University of Turku, and High Performance Language Technologies (HPLT). Training was conducted on the LUMI supercomputer, using compute resources generously provided by CSC - IT Center for Science, Finland.\n",
      "\n",
      "This project is part of an ongoing effort to create open source large language models for non-English and especially low resource languages like Finnish. Through the combination of English and Finnish training data we get a model that outperforms previous Finnish only models, while also being fluent in English and code, and capable of basic translation between English and Finnish.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# configure Client instance with API URL and access token\n",
    "token_source = StaticAccessTokenSource(api_key)\n",
    "aitta_client = Client(\"https://api-staging-aitta.2.rahtiapp.fi\", token_source)\n",
    "\n",
    "# load the LumiOpen/Poro-34B-chat model\n",
    "poro_model = Model.load(\"LumiOpen/Poro-34B-chat\", aitta_client)\n",
    "print(poro_model.description)\n",
    "\n",
    "# configure OpenAI client to use the Aitta OpenAI compatibility endpoints\n",
    "client = openai.OpenAI(api_key=token_source.get_access_token(), base_url=poro_model.openai_api_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08ba7d20-6ef0-4b7b-b8e8-1fb3f2895d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mock dataset as a list of \"documents\"\n",
    "documents = [\"Cacapapadadas are grey, 10cm long worms.\",\n",
    "\"The moon is actually made of a soft cheese.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5d9505f-70e3-49a5-bebd-c705e703f33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#  Initialize the SentenceTransformer model as encoder and generate vector embeddings\n",
    "encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "vectors = encoder.encode(documents)\n",
    "#type(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e9041a7e-a6bd-4f2b-90ae-b1d09d194f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 384)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a9d835-7819-4a5b-953c-befcf52c4b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a4c51f8-94f4-47bb-bdef-f252bafdcb8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "faiss.swigfaiss_avx512.IndexFlatIP"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a FAISS index from vectors\n",
    "\n",
    "import faiss\n",
    "\n",
    "# Determine the dimensionality of the vector embeddings\n",
    "vector_dimension = vectors.shape[1]\n",
    "\n",
    "# Initialize FAISS index using the Inner Product (IP) method for similarity search\n",
    "index = faiss.IndexFlatIP(vector_dimension)  # Using IP for cosine similarity search\n",
    "# Alternatively, you could use IndexFlatL2 for Euclidean distance-based similarity\n",
    "\n",
    "\n",
    "# Normalize the vectors for better performance in cosine similarity\n",
    "faiss.normalize_L2(vectors) # SHOULD THIS BE SOMETHING ELSE??\n",
    "\n",
    "\n",
    "# Add the vectors to the FAISS index\n",
    "index.add(vectors)\n",
    "\n",
    "# Check the type of the index to ensure it's properly created\n",
    "type(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e711487-b731-4d80-abb2-f6e1ed708e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.69921315 0.16020189]]\n",
      "[[1 0]]\n"
     ]
    }
   ],
   "source": [
    "# Create a search vector\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define the query text for searching in the FAISS index\n",
    "search_text = 'What is the moon made of?'\n",
    "\n",
    "# Convert the query text into an embedding (vector)\n",
    "search_vector = encoder.encode(search_text)\n",
    "\n",
    "# Convert the query embedding into a NumPy array and normalize it\n",
    "search_vector = np.array([search_vector])\n",
    "faiss.normalize_L2(search_vector)\n",
    "\n",
    "# Perform a search in the FAISS index to find the most similar document\n",
    "# We search for 'k' nearest neighbors (k=2 for the top 2 results)\n",
    "k = index.ntotal  # We set k to the total number of documents to see how similar all are to the query\n",
    "distances, indices = index.search(search_vector, k=k)  # Perform the search\n",
    "\n",
    "\n",
    "# Print the distances and corresponding indices of the retrieved documents\n",
    "print(distances)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a985a5-47c3-42b6-96ab-f89df6072299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1:\n",
      "Text: The moon is actually made of a soft cheese.\n",
      "Distance: 0.69921315\n",
      "--------------------------------------------------\n",
      "Rank 2:\n",
      "Text: Cacapapadadas are grey, 10cm long worms.\n",
      "Distance: 0.16020189\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print each of the retrieved documents along with their similarity distance\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    print(f\"Rank {i+1}:\")\n",
    "    print(\"Text:\", documents[idx])  # Retrieve the document text by its index\n",
    "    print(\"Distance:\", distances[0][i])  # The distance represents similarity (lower means more similar)\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a48baeb-6b92-405b-a942-e5b03dcca1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cacapapadadas are grey, 10cm long worms.']\n",
      "Most similar document index: [[0]]\n",
      "Distance: [[0.6765009]]\n"
     ]
    }
   ],
   "source": [
    "input_query = \"What are Cacapapadadas?\"\n",
    "\n",
    "\n",
    "# Embed the query\n",
    "query_embedding = encoder.encode(input_query)\n",
    "\n",
    "query_embedding = np.array([query_embedding]) # without this comed IndexError: tuple index out of range\n",
    "faiss.normalize_L2(query_embedding)\n",
    "\n",
    "# Perform similarity search on the FAISS index\n",
    "k = 1  # Number of nearest neighbors to retrieve\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "# Retrieve the document(s) corresponding to the top index\n",
    "retrieved_documents = [documents[i] for i in indices[0]]\n",
    "print(retrieved_documents)\n",
    "\n",
    "# Retrieve the most similar document(s)\n",
    "print(\"Most similar document index:\", indices)\n",
    "print(\"Distance:\", distances)\n",
    "\n",
    "\n",
    "# Prepare the prompt\n",
    "prompt = f\"Given the following document, answer the question:\\n\\nDocument: {retrieved_documents}\\n\\nQuestion: {input_query}\\nAnswer:\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f3f9b63-7e2c-4a9b-af93-c4d6e78bb337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are Cacapapadadas?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d207922e-f52f-469b-94b0-7f0c4b3ef0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the OpenAI API\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ],\n",
    "    model=poro_model.id,\n",
    "    stream=False  # response streaming is currently not supported by Aitta, now you get the full response in one go\n",
    ")\n",
    "\n",
    "# Display the answer\n",
    "answer = response.choices[0].message.content\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac46d83",
   "metadata": {},
   "source": [
    "## LLM usage without RAG\n",
    "\n",
    "Now, let's test how the model responds to the query without relying on an external data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf65854-27bf-446e-9cd9-aeb5ceef10ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Cacapapadada is a fictional character from the animated series \"Archer\", voiced by Jessica Walter. She is the mother of the main character, Archer, and is known for her strict and demanding personality.\n"
     ]
    }
   ],
   "source": [
    "input_query = \"What are Cacapapadadas?\"\n",
    "\n",
    "\n",
    "# Call the OpenAI API\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": input_query\n",
    "        }\n",
    "    ],\n",
    "    model=poro_model.id,\n",
    "    stream=False  # response streaming is currently not supported by Aitta, now you get the full response in one go\n",
    ")\n",
    "\n",
    "# Display the answer\n",
    "answer = response.choices[0].message.content\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e297b56",
   "metadata": {},
   "source": [
    "## Did the model hallucinate? \n",
    "\n",
    "You may notice that the model generates a response based on patterns in the training data, which could be inaccurate. To reduce the chances of hallucination, we can provide a more specific prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fc3041-4ebb-4917-a134-a9747376dff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_query = \"What are Cacapapadadas?\"\n",
    "\n",
    "prompt = f\"Answer the query only if you know the answer for sure. Query: {input_query}\"\n",
    "\n",
    "# Call the OpenAI API\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ],\n",
    "    model=poro_model.id,\n",
    "    stream=False  # response streaming is currently not supported by Aitta, now you get the full response in one go\n",
    ")\n",
    "\n",
    "# Display the answer\n",
    "answer = response.choices[0].message.content\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f1bcf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
