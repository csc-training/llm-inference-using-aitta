{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96212a81-6119-490a-8e93-3e6c571ee452",
   "metadata": {},
   "source": [
    "# Simple RAG example using Facebook AI Similarity Search (FAISS)\n",
    "\n",
    "In this example, we'll demonstrate how to use [**FAISS**](https://faiss.ai/) for similarity-based document retrieval. We will simulate a small **mock dataset** of fictional documents and use **Sentence Transformers** to encode them into vectors. We will then build an **FAISS index** to enable fast and efficient similarity search. Finally, we will simulate a **RAG** system where we retrieve the most relevant documents and use them to generate an answer.\n",
    "\n",
    "\n",
    "## Steps Overview:\n",
    "1. **Create a mock dataset**: We create a small set of fictional documents.\n",
    "2. **Generate embeddings**: We use the **SentenceTransformer** model to convert these documents into vector embeddings.\n",
    "3. **Build FAISS index**: We build a **FAISS index** that will store these vector embeddings for fast similarity search.\n",
    "4. **Search and retrieve**: We perform a similarity search based on a query and retrieve the most relevant document(s).\n",
    "5. **Answer generation**: Using the retrieved document(s), we simulate a **RAG pipeline** to generate a response using OpenAI compatible API Aitta provides.\n",
    "\n",
    "This example demonstrates how **FAISS** can be used for efficient document retrieval, and how **RAG** can help generate contextually relevant answers from these documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f62c690-e7f0-4686-859c-c270448e47de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import openai\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from aitta_client import Model, Client, StaticAccessTokenSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0045d3e-dc0f-4a32-9eff-615a6e6c9e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"<API-KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9617f8-f840-48d3-96b2-d5c7ce2416bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure Client instance with API URL and access token\n",
    "token_source = StaticAccessTokenSource(api_key)\n",
    "aitta_client = Client(\"https://api-staging-aitta.2.rahtiapp.fi\", token_source)\n",
    "\n",
    "# load the LumiOpen/Poro-34B-chat model\n",
    "poro_model = Model.load(\"LumiOpen/Poro-34B-chat\", aitta_client)\n",
    "print(poro_model.description)\n",
    "\n",
    "# configure OpenAI client to use the Aitta OpenAI compatibility endpoints\n",
    "client = openai.OpenAI(api_key=token_source.get_access_token(), base_url=poro_model.openai_api_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ba7d20-6ef0-4b7b-b8e8-1fb3f2895d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mock dataset as a list of \"documents\"\n",
    "documents = [\"Cacapapadadas are grey, 10cm long worms.\",\n",
    "\"The moon is actually made of a soft cheese.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d9505f-70e3-49a5-bebd-c705e703f33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#  Initialize the SentenceTransformer model as encoder and generate vector embeddings\n",
    "encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "vectors = encoder.encode(documents)\n",
    "#type(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9041a7e-a6bd-4f2b-90ae-b1d09d194f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4c51f8-94f4-47bb-bdef-f252bafdcb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a FAISS index from vectors\n",
    "\n",
    "# Determine the dimensionality of the vector embeddings\n",
    "vector_dimension = vectors.shape[1]\n",
    "\n",
    "# Initialize FAISS index using the Inner Product (IP) method for cosine similarity search\n",
    "index = faiss.IndexFlatIP(vector_dimension)  # Using IP for cosine similarity search\n",
    "# Alternatively, you could use IndexFlatL2 for Euclidean distance-based similarity\n",
    "\n",
    "\n",
    "# Normalize the vectors for better performance in cosine similarity\n",
    "faiss.normalize_L2(vectors)\n",
    "\n",
    "\n",
    "# Add the vectors to the FAISS index\n",
    "index.add(vectors)\n",
    "\n",
    "# Check the type of the index to ensure it's properly created\n",
    "type(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e711487-b731-4d80-abb2-f6e1ed708e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a search vector\n",
    "\n",
    "# Define the query text for searching in the FAISS index\n",
    "search_text = 'What is the moon made of?'\n",
    "\n",
    "# Convert the query text into an embedding (vector)\n",
    "search_vector = encoder.encode(search_text)\n",
    "\n",
    "# Convert the query embedding into a NumPy array and normalize it\n",
    "search_vector = np.array([search_vector])\n",
    "faiss.normalize_L2(search_vector)\n",
    "\n",
    "# Perform a search in the FAISS index to find the most similar document\n",
    "# We search for 'k' nearest neighbors (k=2 for the top 2 results)\n",
    "k = index.ntotal  # We set k to the total number of documents to see how similar all are to the query\n",
    "distances, indices = index.search(search_vector, k=k)  # Perform the search\n",
    "\n",
    "\n",
    "# Print the distances and corresponding indices of the retrieved documents\n",
    "print(distances)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a985a5-47c3-42b6-96ab-f89df6072299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print each of the retrieved documents along with their similarity distance\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    print(f\"Rank {i+1}:\")\n",
    "    print(\"Text:\", documents[idx])  # Retrieve the document text by its index\n",
    "    print(\"Distance:\", distances[0][i])  # The distance represents similarity (lower means more similar)\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a48baeb-6b92-405b-a942-e5b03dcca1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_query = \"What are Cacapapadadas?\"\n",
    "\n",
    "\n",
    "# Embed the query\n",
    "query_embedding = encoder.encode(input_query)\n",
    "\n",
    "query_embedding = np.array([query_embedding]) # without this comed IndexError: tuple index out of range\n",
    "faiss.normalize_L2(query_embedding)\n",
    "\n",
    "# Perform similarity search on the FAISS index\n",
    "k = 1  # Number of nearest neighbors to retrieve\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "# Retrieve the document(s) corresponding to the top index\n",
    "retrieved_documents = [documents[i] for i in indices[0]]\n",
    "print(retrieved_documents)\n",
    "\n",
    "# Retrieve the most similar document(s)\n",
    "print(\"Most similar document index:\", indices)\n",
    "print(\"Distance:\", distances)\n",
    "\n",
    "\n",
    "# Prepare the prompt\n",
    "prompt = f\"Given the following document, answer the question:\\n\\nDocument: {retrieved_documents}\\n\\nQuestion: {input_query}\\nAnswer:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3f9b63-7e2c-4a9b-af93-c4d6e78bb337",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d207922e-f52f-469b-94b0-7f0c4b3ef0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the OpenAI API\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ],\n",
    "    model=poro_model.id,\n",
    "    stream=False  # response streaming is currently not supported by Aitta, now you get the full response in one go\n",
    ")\n",
    "\n",
    "# Display the answer\n",
    "answer = response.choices[0].message.content\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac46d83",
   "metadata": {},
   "source": [
    "## LLM usage without RAG\n",
    "\n",
    "Now, let's test how the model responds to the query without relying on an external data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf65854-27bf-446e-9cd9-aeb5ceef10ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_query = \"What are Cacapapadadas?\"\n",
    "\n",
    "\n",
    "# Call the OpenAI API\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": input_query\n",
    "        }\n",
    "    ],\n",
    "    model=poro_model.id,\n",
    "    stream=False  # response streaming is currently not supported by Aitta, now you get the full response in one go\n",
    ")\n",
    "\n",
    "# Display the answer\n",
    "answer = response.choices[0].message.content\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e297b56",
   "metadata": {},
   "source": [
    "## Did the model hallucinate? \n",
    "\n",
    "You may notice that the model generates a response based on patterns in the training data, which could be inaccurate. To reduce the chances of hallucination without utilizing RAG, we can try to provide a more specific prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fc3041-4ebb-4917-a134-a9747376dff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_query = \"What are Cacapapadadas?\"\n",
    "\n",
    "prompt = f\"Answer the query only if you know the answer for sure. Query: {input_query}\"\n",
    "\n",
    "# Call the OpenAI API\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ],\n",
    "    model=poro_model.id,\n",
    "    stream=False  # response streaming is currently not supported by Aitta, now you get the full response in one go\n",
    ")\n",
    "\n",
    "# Display the answer\n",
    "answer = response.choices[0].message.content\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f1bcf4",
   "metadata": {},
   "source": [
    "## Would you like to see a more advanced example?\n",
    "\n",
    "Check out the [repository](https://github.com/shanshanwangcsc/simple_chatbot/tree/aitta_integration) on AITTA integration with the RAG pipeline for a simple chatbot implementation. This example uses ChromaDB for vector storage and retrieval, with the LLM being accessed through LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21dd593",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
