{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Large Language Models (LLMs)\n",
    "\n",
    "## What are large language models? \n",
    "\n",
    "LLMs are AI systems that use deep learning algorithms to process and generate human-like text. From chatbots to automatic translation and content creation, LLMs are revolutionizing how we interact with technology. They function by receiving an input (prompt), for which they iteratively predict the most probable next words and thus produce a continuation of the prompt in natural language. Fine-tuned chat models are optimized to produce responses that align with user requests. Generation proceeds until the model emits a stop token or reaches a predetermined length limit. These models have been trained to predict the next words in a sentence based on massive amounts of training data, enabling them to generate coherent and contextually relevant responses.\n",
    "\n",
    "**Key capabilities of LLMs:**\n",
    "* **Text prediction:** Generating coherent text based on input.\n",
    "* **Summarization:** Condensing long documents into concise summaries.\n",
    "* **Translation:** Converting text between languages.\n",
    "* **Content generation:** Writing articles, stories or even code.\n",
    "\n",
    "\n",
    "## Training phases \n",
    "\n",
    "Training an LLM is a two-step process: pre-training and fine-tuning. Both phases are essential for building an effective model that can understand and generate human-like text.\n",
    "\n",
    "The image below illustrates the high-level data flow in LLM training phases.\n",
    "\n",
    "![pretraining-finetuning](./images/pretraining-finetuning.png)\n",
    "\n",
    "\n",
    "### Pre-training: learning patterns from data\n",
    "\n",
    "During the self-supervised pre-training phase, LLMs analyze massive amounts of text to recognize linguistic patterns. The training data typically comes from diverse sources such as websites, online books, research papers, and code repositories. The resulting pre-trained models can generate text completions but lack fine-grained task awareness. Pre-training datasets often lack domain-specific knowledge, requiring further refinement. Sometimes pre-trained models can be difficult to use and produce harmful outputs. Pre-trained models reflect the data they are trained on, meaning they may contain biases or produce unexpected outputs. Since pre-training covers general language patterns rather than specific tasks, additional fine-tuning is often required to align the model with real-world applications and ethical considerations. \n",
    "\n",
    "*Note: Pre-trained models are sometimes referred to as 'base models.' However, 'pre-trained model' is sometimes used to mean an instruction-tuned model, which is then further fine-tuned with domain-specific data.*\n",
    "\n",
    "### Fine-tuning: adapting to specific tasks\n",
    "\n",
    "Fine-tuning is a set of techniques used to modify a pre-trained model to improve its performance on specific tasks. This involves training the model on specialized datasets to refine its responses. \n",
    "\n",
    "Types of fine-tuning:\n",
    "* **Instruction-tuning**: The model is trained with input-output pairs to follow specific instructions, often enhanced with Reinforcement Learning from Human Feedback (RLHF) to improve its ability to understand and respond to prompts effectively.\n",
    "* **Chat-tuning**: Fine-tuning the model to perform well in conversational contexts, improving its ability to engage in natural, coherent dialogue.\n",
    "* **Reinforcement Learning from Human Feedback (RLHF)**: Improves responses based on human evaluations, refining the model's behavior and reducing harmful or biased outputs.\n",
    "* **Task-specific tuning**: Adapting models for applications like question-answering, summarization, and sentiment analysis.\n",
    "\n",
    "\n",
    "### **Pre-training vs. Fine-tuning**\n",
    "| Feature | Pre-training | Fine-tuning |\n",
    "| ------- | ------------ | ----------- |\n",
    "| Purpose | Learning general language patterns | Adapting to specific tasks |\n",
    "| Training data | Massive, diverse text datasets | Task-specific datasets |\n",
    "| Result | Base model with broad understanding | Improved performance for targeted applications | \n",
    "\n",
    "\n",
    "## Evaluation after training \n",
    "\n",
    "Evaluating an LLM is essential to ensure it produces accurate, reliable, and unbiased results. While there are many ways to assess a model’s performance, this course does not dive deeply into evaluation methods. Instead, we focus on how to use LLMs effectively in practical applications.\n",
    "\n",
    "For those interested in learning more, there are many resources available on model evaluation techniques, such as accuracy testing, bias detection, and human feedback assessment. For further reading, see this [\"LLM Evaluation guidebook\"](https://github.com/huggingface/evaluation-guidebook) hosted by Hugging Face. \n",
    "\n",
    "## Understanding opensource LLMs\n",
    "\n",
    "Many LLMs are available as open-source models, each with different licenses and capabilities.\n",
    "[Hugging Face](https://huggingface.co/) is a platform where researchers and developers share open-source LLMs, fine-tuned models and datasets. Users can browse the [Model Hub](https://huggingface.co/models) to find models suited for various tasks and run them eg. locally with tools like the `transformers` library. Developers can also easily integrate and fine-tune these models for specific applications.\n",
    "\n",
    "\n",
    "**Model performance depends on several key factors:**\n",
    "\n",
    "* **Training data:** \n",
    "    * The dataset quality directly impacts accuracy, biases, and domain knowledge.\n",
    "* **Optimization for applications:**\n",
    "    * Some models are fine-tuned for tasks like summarization, translation, or chatbot interactions.\n",
    "* **Model size & parameters:**\n",
    "    * More parameters typically improve coherence and contextual understanding.\n",
    "    * Larger models require higher computational resources, increasing memory demands and processing costs.\n",
    "    * Techniques such as reduced floating point precision (e.g., using bfloat16 instead of float32) and quantization can reduce model size and computational requirements without decreasing the number of parameters.\n",
    "\n",
    "Additionally, it’s important to note that each model has a defined context length, which is the maximum number of tokens it can process at once. This limitation can impact the model's ability to generate coherent responses. Models with shorter context lengths may struggle with longer inputs, which could result in truncated responses or loss of context.\n",
    "\n",
    "Since an LLM is only as good as its training data, it’s essential to review which datasets were used in its development. Understanding a model’s architecture, dataset composition, and licensing can help determine its suitability for specific applications.\n",
    "\n",
    "\n",
    "\n",
    "### Example: LumiOpen/Poro-34B models\n",
    "\n",
    "The `LumiOpen/Poro-34B` model, developed by Finnish institutions and Silo AI, is an example of an open-source pre-trained LLM. It was trained using the LUMI supercomputer and supports Finnish, English, and code generation. This base model has been fine-tuned into a chat model: `LumiOpen/Poro-34B-Chat`. These are some of the first models available through Aitta. \n",
    "\n",
    "**Poro-34B-Chat excels at:**\n",
    "* Answering questions in Finnish and English\n",
    "* Following instructions in plain language\n",
    "* Generating high-quality translations\n",
    "* Writing code and assisting with development tasks\n",
    "\n",
    "\n",
    "| Model | Features | \n",
    "| ------- | ------------ | \n",
    "| Poro-34B | A foundational GPT (Generative pre-trained transformer) model trained on diverse datasets. |\n",
    "| Poro-34B-Chat | A fine-tuned version optimized for conversational AI, capable of answering questions, following instructions, and generating high-quality translations. |\n",
    "\n",
    "You can view these models in Hugging Face's Model Hub, visit https://huggingface.co/LumiOpen and choose a model to view it's Model Card. You can also explore the research paper [\"Poro 34B and the Blessing of Multilinguality\"](https://arxiv.org/pdf/2404.01856).\n",
    "\n",
    "\n",
    "#### Example interactions with Poro-34B-Chat:\n",
    "\n",
    "The images above show two responses from the `LumiOpen/Poro-34B-Chat` model using the same input (prompt). \n",
    "Default parameters were used, meaning that settings like the length-determining parameter `max_completion_tokens` were not modified.\n",
    "\n",
    "\n",
    "Take a look — do you find these answers reliable?\n",
    "\n",
    "\n",
    "Example 1             |  Example 2\n",
    ":-------------------------:|:-------------------------:\n",
    "![](./images/poro_chat_example1.jpg)  |  ![](./images/poro_chat_example2.jpg)\n",
    "\n",
    "In the first example, the model incorrectly states that \"LLM\" stands for Language Learning Models. In the second, the response is cut off mid-sentence. \n",
    "\n",
    "While LLM responses are not always factually accurate, LLMs are still powerful tools for various tasks. They can generate text, summarize information, assist with coding, translate languages and much more. The key is to understand their limitations and verify critical information when needed.\n",
    "\n",
    "### Conclusion \n",
    "\n",
    "LLMs are powerful AI tools, but their effectiveness depends on the quality of their training data, fine-tuning techniques and hardware capabilities. As one of the European AI Factories LUMI AI Factory (LAIF) is working on improving access to training datasets and expanding accessibility to open-source AI models, helping to accelerate LLM development. One of the tools LAIF provides is the AI inference platform Aitta. Before we dive into Aitta, let's explore inference — how trained models generate responses efficiently and accurately in real-world applications.\n",
    "\n",
    "You can move on to [**Inference - Using Trained LLMs**](./02_inference.ipynb).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
