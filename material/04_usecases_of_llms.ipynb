{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Usecases of LLMs\n",
    "\n",
    "While building your own LLM from scratch can be complex and resource-intensive, many high-quality models are available to the public. By leveraging these models, researchers, developers, and businesses can harness advanced language capabilities without the complexity of training their own models.\n",
    "\n",
    "Before using an LLM, it's essential to clearly define your goal — whether it's debugging code, classifying text, or summarizing documents. There are multiple ways to access and use these models:\n",
    "\n",
    "* Open-source models (e.g., on Hugging Face) allow full customization and deployment in any environment.\n",
    "* Paid API-based services provide access to cutting-edge models without infrastructure setup.\n",
    "* User-friendly interfaces enable quick experimentation without coding.\n",
    "\n",
    "Since we have familiarised ourselves with Aitta by now, we will continue using it in our coding exercises. \n",
    "\n",
    "Once you’ve chosen a model, the next step is to experiment with it. Experimenting with prompting an LLM in different ways is a good way to understand its capabilities and whether it can perform the desired tasks to a satisfactory level. Many models are released with model cards, which are a great resource for accessing and understanding the details of specific models. With some iteration, it is possible to produce specific inputs or prompts that give helpful outputs. For example, a researcher might ask a model to summarise a longer document or translate text into another language.\n",
    "\n",
    "\n",
    "## Prompt engineering & optimization\n",
    "\n",
    "Prompt engineering involves designing structured inputs that guide an LLM toward producing more accurate, relevant, and useful responses. Since LLMs generate text based on probability rather than true understanding, the way a prompt is phrased significantly affects the output.\n",
    "\n",
    "A well-structured prompt helps:\n",
    "* Improve response accuracy\n",
    "* Reduce hallucinations (false or misleading information)\n",
    "* Control the style, tone, and format of the output\n",
    "\n",
    "**Examples of effective prompting** \n",
    "  \n",
    "\n",
    "| Prompting technique       | Bad example                                                | Improved example                                                 |  \n",
    "|---------------------------|------------------------------------------------------------|------------------------------------------------------------------|  \n",
    "| Be specific               | \"Summarize this.\"                                          | \"Summarize this article in 3 bullet points with key findings.\"   |  \n",
    "| Provide context            | \"Explain deep learning.\"                                  | \"Explain deep learning as if I'm a biology researcher new to AI.\" |  \n",
    "| Define output format      | \"List key insights.\"                                       | \"List 3 key insights in markdown bullet format.\"                 |  \n",
    "| Use role-based prompts    | \"Write a research summary.\"                                | \"You are a research assistant. Summarize this paper for a PhD audience.\" |  \n",
    "\n",
    "\n",
    "Beyond these basic techniques lie more sophisticated approaches: There are advanced prompting strategies like few-shot prompting or chain-of-thought prompting. You are free to search for more information about these advanced techniques if you like. One option is [**Prompt Engineering Guide**](https://www.promptingguide.ai/), made by [DAIR.AI](https://dair.ai/about/).\n",
    "\n",
    "## Real life usecase\n",
    "\n",
    "LLMs are widely used across industries, enabling powerful automation and augmentation of tasks. Here are some real-world applications:\n",
    "\n",
    "* **Academic research**: Use LLMs to summarize papers, extract key insights, and even generate literature reviews.\n",
    "* **Healthcare**: Process large volumes of medical texts to extract relevant findings for practitioners.\n",
    "* **Customer support**: Automate answering customer inquiries using chatbots with natural responses.\n",
    "* **Coding assistance**: Debug and explain code snippets, speeding up software development.\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "## Exercise time!\n",
    "\n",
    "Coding exercises are available through [CSC's Noppe service](https://noppe.2.rahtiapp.fi/welcome).\n",
    "\n",
    "\n",
    "### Exercise 4: Prompt testing\n",
    "\n",
    "* Using some other model, eg. OLMo, let's compare responses using different kind of prompts\n",
    "* Choose a chat-tuned model and use it to test prompts **[04_prompt-testing.ipynb](../exercises/04_prompt-testing.ipynb)**\n",
    "\n",
    "### Exercise 5: Real usecases for chat-tuned models\n",
    "\n",
    "* Explore few short examples of real-life usecases for LLMs **[05_usecases_Poro-34B-chat.ipynb](../exercises/05_usecases_Poro-34B-chat.ipynb)**\n",
    "\n",
    "\n",
    "### Exercise 6: Simple example for multiple abstract summarization\n",
    "\n",
    "* Lets use `allenai/OLMo-7B-0724-Instruct` model for summarizing multiple abstracts  **[06_summarizate-abstracts.ipynb](../exercises/06_summarizate-abstracts.ipynb)**\n",
    "\n",
    "----\n",
    "\n",
    "## Challenges in LLM inference\n",
    "\n",
    "LLMs may generate false or misleading information while sounding confident. This is called hallucination. This happens because they predict text based on probable next words (in the context of previous words) rather than a factual understanding of the world. LLMs typically can reproduce knowledge contained in their original training data with some accuracy but do not have access to live or up-to-date information unless connected to external sources.\n",
    "\n",
    "There are several techniques to mitigate these problems. As already mentioned, prompt engineering and fine-tuning can help improve response accuracy by providing a more accurate context for the model when it interprets inputs. A more advanced approach, RAG (Retrieval-Augmented Generation), allows LLMs to pull relevant, real-time information from external knowledge bases, reducing hallucinations and improving factual accuracy.\n",
    "\n",
    "### RAG applications\n",
    "\n",
    "RAG systems combine the power of information retrieval and language generation. They first search through a collection of documents to find the most relevant ones to a given query, and then use a language model to generate accurate and contextually relevant responses based on the retrieved information. This approach enhances the model’s ability to provide more precise answers by grounding the generation in external data.\n",
    "\n",
    "Aitta enables these uses for a number of models and can be used to build a RAG system using e.g. the `langchain` library. One concrete example of RAG application integrated with Aitta can be found here: [**simple_chatbot**](https://github.com/shanshanwangcsc/simple_chatbot/tree/aitta_integration). \n",
    "\n",
    "Below is an illustration of the data flow in RAG applications. The diagram highlights the steps involved in data indexing, retrieval, and generation. In the upcoming notebook exercises, we will take a closer look at each of these steps in detail.\n",
    "\n",
    "<img src=\"./images/rag-pipeline-v2.png\" width=\"700\" height=\"500\"/>\n",
    "\n",
    "\n",
    "### EXTRA EXERCISES:\n",
    "\n",
    "\n",
    "### Exercise 7: Basics of RAG\n",
    "\n",
    "* Explore all the steps required to build RAG assisted LLM: **[07_rag_basics.ipynb](../exercises/07_rag_basics.ipynb)**\n",
    "\n",
    "\n",
    "### Exercise 8: Simple RAG example using FAISS\n",
    "\n",
    "* Let's explore RAG with a FAISS index: **[08_simpleRAG-using-FAISS.ipynb](../exercises/08_simpleRAG-using-FAISS.ipynb)**\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "## We would appreciate your feedback!\n",
    "\n",
    "Please take a moment to complete [feedback survey](https://link.webropolsurveys.com/S/8E03D8B03D0E5336). Your insights help us improve our course material and exercises.\n",
    "\n",
    "**Now you have completed this concise course! &#x1F44F;**  \n",
    "**Thank you &#x1F60A;**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
